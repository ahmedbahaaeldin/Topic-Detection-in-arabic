{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xszq-HWgQqYS"
   },
   "source": [
    "# **For Downloading a dataset for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Bjf3E1OtlqFd"
   },
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/ailab-uniud/akec.git\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WX74HVysQ2n2"
   },
   "source": [
    "# **Testing Connection with Cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3L5vNj4wl_FJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPAMPrKGQ8q1"
   },
   "source": [
    "# **Downloading the Needed Libraries for the code to run properly**\n",
    "\n",
    "\n",
    "1.   Gensim for Word2Vec model loading download from github https://github.com/bakrianoo/aravec , Twitter 300 CBOW.\n",
    "Found on the drive link :\n",
    "https://drive.google.com/open?id=1ubnmvIOqDLgoK7zZu-_UDvNwqMmvLcWO\n",
    "2.   Keras for the deep learning models and text preprocessing\n",
    "3.   Sklearn for the evaluation of the model metrics\n",
    "4.   UnicodeCsv for reading and editing the arabic language DataSet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "z4cJAxmTywxC"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install keras\n",
    "!pip install sklearn\n",
    "!pip install unicodecsv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAZn17wMR5C5"
   },
   "source": [
    "# The Following Two Cells responsible for the connection with your own drive where the data is loaded for people who use Google Colab where upload and download is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M1WakuvfFHq5"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iINKxp8T0fDj"
   },
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJFiXR6zSGAO"
   },
   "source": [
    "# **The imported Libraries and packages to be used in the whole notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XVDuP_nChKCb"
   },
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import unicodecsv as csv\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential,Model,model_from_json\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Flatten, TimeDistributed,Conv1D,MaxPooling1D,Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqv2CMYxSOZD"
   },
   "source": [
    "# Total is an array where the whole dataset is loaded \n",
    "Opening the file with read byte type for arabic language and encoding utf-8, errors=ignore for strange characters to continue readng the file and donot break.\n",
    "The process in appending is made 0:2 only taking first two columns as some documents exceed the limit of CSV file format and take more than two cells , all with category economics as humanly observed and revised. This problem will be dealt with soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bkh8UTZPZyZ3",
    "outputId": "e8f05f17-23b2-4870-e279-dad9fd5f8a11"
   },
   "outputs": [],
   "source": [
    "Total=[]\n",
    "\n",
    "with open(u'drive/LSTM-DS.csv', \"rb\") as csvfile:\n",
    "    reader = csv.reader(csvfile, encoding=\"utf-8\",errors='ignore')\n",
    "    \n",
    "\n",
    "    for row in reader:\n",
    "       \n",
    "       Total.append(row[0:2]) \n",
    "\n",
    "random.shuffle(Total)\n",
    "print(len(Total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpTr_kjISzzv"
   },
   "source": [
    "# Making sure data is read correctly with no errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "4peP_mEY3M0N",
    "outputId": "9a5e09cb-8956-42ce-85c8-02814049bc5b"
   },
   "outputs": [],
   "source": [
    "print(Total[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PjwDkp_S8go"
   },
   "source": [
    "# Excluding sequences that has more than 80 words and less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "H2Z6bNvlbmA2"
   },
   "outputs": [],
   "source": [
    "for line in Total:\n",
    "  if len(line[0].split(\" \"))>80:\n",
    "    Total.remove(line)\n",
    "  if len(line[0].split(\" \"))<5:\n",
    "    Total.remove(line)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPaYUSlETGRb"
   },
   "source": [
    "# **Removing Lines with no labels then appending the sequences to dataset array and labels into labels array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XVdHxsUmaCr0"
   },
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "labels=[]\n",
    "for line in Total:\n",
    "  if len(line[1].split(\" \"))>1:\n",
    "    Total.remove(line)\n",
    "  else:\n",
    "    dataset.append(line[0])\n",
    "    labels.append(line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTS-r-jRTRtW"
   },
   "source": [
    "# **Spliting the sequences into words for training , as each word will correspond to a vector, then printing the total size after preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4OsyH3lLZ7tu",
    "outputId": "93625d12-1cd8-4d5e-b44c-8badf7800921"
   },
   "outputs": [],
   "source": [
    "X_total=[i.split(\" \") for i in dataset]\n",
    "print(len(Total))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MyUrMhfTfjy"
   },
   "source": [
    "# **Defining the maximum number of different words available and the tokenizer package in keras which allows us to map words into their position in the whole dataset which is easier to retrieve and faster to process **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "3YJID4jZZBQK",
    "outputId": "d5206719-ec51-493b-981d-11ed72f7f71e"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS=495145\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_total)\n",
    "sequences = tokenizer.texts_to_sequences(X_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOg9-nw_TwsN"
   },
   "source": [
    "# **Defining the number of labels we have , 5 categories and mapping the labels to numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n2UfUQc3qAGw",
    "outputId": "fae1c675-3830-41f0-890b-d28970f34e77"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "label=[]\n",
    "label_index={'diverse':0,'news':1,'sports':2,'economics':3,'culture':4}\n",
    "\n",
    "for word in labels:\n",
    " \n",
    "  if label_index.get(word):\n",
    "    label_id=label_index[word]\n",
    "    label.append(label_id)\n",
    "  else:\n",
    "    label_id=3\n",
    "    label.append(label_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzFA9hRpT3pi"
   },
   "source": [
    "# **Padding the sequences to all have the same sequence of 80 words maximum per sequence. Then printing the shape of our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "uq317k_CNICt",
    "outputId": "3b1b660a-f264-4844-9867-93847e2e5874"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=80\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "label = to_categorical(np.asarray(label))\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', label.shape)\n",
    "print(label[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmDyaYiRUBWk"
   },
   "source": [
    "# Again Making sure everything is working :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "5mkuihpqjYwN",
    "outputId": "c3969b90-9e85-41c4-9586-c2de3bde71dd"
   },
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jy8DwYjvUHSO"
   },
   "source": [
    "# **Spliting the data into training data and validation data with 80% , 20% portion respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E8RNR8zfrPJB",
    "outputId": "0ce6ab77-39a4-4f4e-dfb7-73704237ef29"
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "label = label[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = label[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = label[-nb_validation_samples:]\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wnI2aZzUQiK"
   },
   "source": [
    "# **Loading Word2Vec model and creating the embedding matrix which is mapped index to word and word to index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "KOebl8W7b09p",
    "outputId": "b966b158-fe5f-416e-aeec-c691698a842c"
   },
   "outputs": [],
   "source": [
    "modelwv=Word2Vec.load(u\"drive/tweet_cbow_300/tweet_cbow_300/tweets_cbow_300\")\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if word in modelwv.wv.vocab:\n",
    "      embedding_vector = modelwv[word]\n",
    "    else:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKiJ9KcDUZsn"
   },
   "source": [
    "# Lastly checking the dimensions of embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zYemt9yKgKwb",
    "outputId": "47f74e9f-6ff5-45ca-d796-f8e5cf82709a"
   },
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)\n",
    "EMBEDDING_DIM=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jOgYK_LUevg"
   },
   "source": [
    "# **Roc Auc Evaluation function but is not very useful in our case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zsJBnF5Y0CTw"
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p5Ea0b6jUltL"
   },
   "source": [
    "# **Finally our Bidirectional LSTM with conv1D model , specs described in the document passed. Learning process will take a while so please be patient :D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "dQc54So2uUa6",
    "outputId": "0b90a328-a8f2-486e-e036-29f7a1e334b3"
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = Bidirectional(LSTM(80, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(40, kernel_size = 5, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool]) \n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "preds = Dense(len(label_index), activation='softmax')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "#filepath=\"drive/weights_base.best.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "#ra_val = RocAucEvaluation(validation_data=(x_val, y_val), interval = 7)\n",
    "#callbacks_list = [ra_val,checkpoint, early]\n",
    "model.fit(x_train, y_train,validation_data=(x_val, y_val), batch_size=100, epochs=4,verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agXUnXLwU3h3"
   },
   "source": [
    "# **Another Model : Conv1D model with MaxPooling of 5x5 and relu activation. And saving the model weights and json file to the drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZOaCuhNJ0Gx3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(80, 1, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(80, 1, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(80, 1, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(80, activation='relu')(x)\n",
    "preds = Dense(len(label_index)+1, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,validation_data=(x_val, y_val),epochs=6,batch_size=50)\n",
    "scores = model.evaluate(x_train,y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "model_json = model.to_json()\n",
    "with open(\"drive/model_conv1d.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"drive/model_conv1dweights.h5\")\n",
    "print(\"Saved model to drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLKHi5TWVG2R"
   },
   "source": [
    "# **Final Model of Bidirectional LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rQHvaSsZF8bH"
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Bidirectional(LSTM(80, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(embedded_sequences)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(len(label_index)+1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=sequence_input, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,validation_data=(x_val, y_val), batch_size=50, epochs=4,verbose=1) # validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilzQvE9gVOud"
   },
   "source": [
    "# **Evaluation the model performance and printing out the accuracy percentage (the model cell run will have its accuracy printed by using this cell whether its BiLSTM with Conv1D or any other)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Gjtey2K6yjDu"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_train,y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3DvkjNbv5Xph",
    "outputId": "b9e3af3a-ca86-4b84-a5e7-92571fe2ec12"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"drive/final_LSTMCnn.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"drive/final_LSTMCnn_Weights.h5\")\n",
    "print(\"Saved model to drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5eqON6p8Vfsz"
   },
   "source": [
    "# **Happy testing, Insert the desired Sentence to be tested in totest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EzjKB_Ytxolb"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=80\n",
    "totest=[\"\"]\n",
    "totest=[i.split(\" \") for i in totest]\n",
    "print(totest)\n",
    "Xnew=np.array(tokenizer.texts_to_sequences(totest))\n",
    "Xnew = pad_sequences(Xnew, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "ynew = model.predict(Xnew)\n",
    "\n",
    "result=np.argmax(ynew,axis=1)\n",
    "for k,v in label_index.items():\n",
    "  if v==result:\n",
    "    print(k)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dokki.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
